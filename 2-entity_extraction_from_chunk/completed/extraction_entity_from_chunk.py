"""
Entity Extraction from Chunk Pipeline
Flow:
1. Extract entities from chunk (LLM)
2. Resolve entity names via OpenSearch (synonym matching)
3. Save entities to Neptune
4. Resolve relationship names using cache
5. Save relationships to Neptune
"""
from langchain_text_splitters import RecursiveCharacterTextSplitter
from utils.parse_utils import parse_extraction_output
from utils.generate_entity import extract_entities
from utils.helper import (
    get_context_from_review_file, 
    get_all_review_files,
    generate_chunk_hash,
    generate_chunk_id
)
from opensearch.opensearch_con import get_opensearch_client
from opensearch.opensearch_search import resolve_entities, resolve_relationships, delete_chunk_index_opensearch
from neptune.cyper_queries import (
    import_nodes_with_dynamic_label,
    import_relationships_with_dynamic_label,
    delete_all_nodes_and_relationships,
    get_database_stats
)


def run_entity_extraction_pipeline(
    reviews_dir: str = None,
    chunk_size: int = 1500,
    chunk_overlap: int = 100,
    clean_database: bool = True,
    save_to_neptune: bool = True
):
    """
    ì „ì²´ ì—”í‹°í‹° ì¶”ì¶œ íŒŒì´í”„ë¼ì¸
    
    Args:
        reviews_dir: ë¦¬ë·° íŒŒì¼ ë””ë ‰í† ë¦¬ (Noneì´ë©´ ê¸°ë³¸ ê²½ë¡œ)
        chunk_size: ì²­í¬ í¬ê¸°
        chunk_overlap: ì²­í¬ ì˜¤ë²„ë©
        clean_database: Neptune DB ì´ˆê¸°í™” ì—¬ë¶€
        save_to_neptune: Neptuneì— ì €ì¥ ì—¬ë¶€
    """
    print("=" * 60)
    print("ğŸš€ Entity Extraction Pipeline Start")
    print("=" * 60)
    
    # OpenSearch ì´ˆê¸°í™”
    opensearch_client = get_opensearch_client()
    print("âœ… OpenSearch connected")
    
    # Neptune ìƒíƒœ í™•ì¸
    if save_to_neptune:
        stats = get_database_stats()
        print(f"ğŸ“Š Neptune: {stats['total_nodes']} nodes, {stats['total_relationships']} relationships")
        
        if clean_database:
            delete_all_nodes_and_relationships()
            print("ğŸ—‘ï¸ Database cleaned")
            delete_chunk_index_opensearch()
    
    # ë¦¬ë·° íŒŒì¼ ê°€ì ¸ì˜¤ê¸°
    if reviews_dir:
        from pathlib import Path
        review_files = list(Path(reviews_dir).rglob("*.json"))
    else:
        review_files = get_all_review_files()
    
    print(f"ğŸ“‚ Found {len(review_files)} review files")
    
    # í…ìŠ¤íŠ¸ ìŠ¤í”Œë¦¬í„°
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    
    # ì „ì²´ í†µê³„
    total = {
        'entities_extracted': 0,
        'entities_matched': 0,
        'entities_new': 0,
        'entities_saved': 0,
        'entities_existing_in_neptune': 0,
        'entities_new_in_neptune': 0,
        'relationships_extracted': 0,
        'relationships_saved': 0,
        'relationships_existing_in_neptune': 0,
        'relationships_new_in_neptune': 0,
        'chunks_processed': 0
    }
    
    for i, review_file in enumerate(review_files, 1):
        print(f"\n{'='*60}")
        print(f"ğŸ“„ [{i}/{len(review_files)}] {review_file.name}")
        print('='*60)
        
        try:
            # íŒŒì¼ì—ì„œ ì •ë³´ ì¶”ì¶œ
            _, transcript, movie_id, reviewer = get_context_from_review_file(str(review_file))
            print(f"   ğŸ¬ Movie: {movie_id}, Reviewer: {reviewer}")
            
            # ì²­í‚¹
            chunks = text_splitter.split_text(transcript)
            print(f"   ğŸ“ Chunks: {len(chunks)}")
            
            for j, chunk in enumerate(chunks, 1):
                print(f"\n   --- Chunk {j}/{len(chunks)} ---")
                print(f"f{chunk[:800]}... ìƒëµ ...")
                print(f"\n   --- Chunk Transformation ---")

                # ì²­í¬ ID ìƒì„±
                chunk_hash = generate_chunk_hash(chunk)
                chunk_id = generate_chunk_id(reviewer, chunk_hash)
                
                # Step 1: LLMìœ¼ë¡œ ì—”í‹°í‹°/ê´€ê³„ ì¶”ì¶œ
                result = extract_entities({"user_query": chunk})
                entities, relationships = parse_extraction_output(result)
                
                total['entities_extracted'] += len(entities)
                total['relationships_extracted'] += len(relationships)
                
                if not entities:
                    print("   âš ï¸ No entities found")
                    total['chunks_processed'] += 1
                    continue
                
                print(f"   ğŸ“ Extracted: {len(entities)} entities, {len(relationships)} relationships")
                
                # Step 2: OpenSearchì—ì„œ ì—”í‹°í‹° ì´ë¦„ í•´ê²°
                resolved_entities, entity_metrics = resolve_entities(entities, opensearch_client)
                
                total['entities_matched'] += entity_metrics['matched']
                total['entities_new'] += entity_metrics['new']
                
                # ì—”í‹°í‹° ë§¤ì¹­ ê²°ê³¼ ì¶œë ¥
                print(f"\n   ğŸ“Š Entity Resolution: Search Entity in Neptune via OpenSearch Synonym")
                for ent in resolved_entities:
                    original = ent.get('_original_name', ent['entity_name'])
                    resolved_name = ent['entity_name']
                    etype = ent['entity_type']
                    matched = ent.get('_matched', False)
                    match_type = ent.get('_match_type', 'not_found')
                    
                    if matched:
                        if original != resolved_name:
                            print(f"      âœ… '{original}' â†’ '{resolved_name}' ({etype}) [{match_type}]")
                        else:
                            print(f"      âœ… '{original}' ({etype}) [{match_type}]")
                    else:
                        print(f"      ğŸ†• '{original}' ({etype}) [NEW]")
                
                # Step 3: Neptuneì— ì—”í‹°í‹° ì €ì¥
                if save_to_neptune and resolved_entities:
                    # ë©”íƒ€ë°ì´í„° ì œê±° í›„ ì €ì¥
                    clean_entities = []
                    for ent in resolved_entities:
                        clean_ent = {k: v for k, v in ent.items() if not k.startswith('_')}
                        clean_entities.append(clean_ent)
                    
                    save_result = import_nodes_with_dynamic_label(clean_entities, movie_id, reviewer, chunk_id, chunk, chunk_hash)
                    entity_stats = save_result.get('stats', {})
                    total['entities_saved'] += entity_stats.get('total', len(clean_entities))
                    total['entities_existing_in_neptune'] += entity_stats.get('existing', 0)
                    total['entities_new_in_neptune'] += entity_stats.get('new', 0)
                    
                    # ê¸°ì¡´/ì‹ ê·œ entity í†µê³„ ì¶œë ¥
                    existing_count = entity_stats.get('existing', 0)
                    new_count = entity_stats.get('new', 0)
                    total_count = entity_stats.get('total', 0)
                    print(f"   ğŸ’¾ Saved {total_count} entities to Neptune (DBì— ê¸°ì¡´ ì¡´ì¬: {existing_count}, ì‹ ê·œ ìƒì„±: {new_count})")
                
                # Step 4 & 5: ê´€ê³„ ì²˜ë¦¬ ë° ì €ì¥
                if relationships:
                    # ê´€ê³„ì˜ ì—”í‹°í‹° ì´ë¦„ë„ í•´ê²°
                    resolved_relationships, rel_metrics = resolve_relationships(
                        relationships, opensearch_client
                    )
                    
                    print(f"\n   ğŸ”— Relationship Resolution:")
                    for rel in resolved_relationships[:5]:  # ì²˜ìŒ 5ê°œë§Œ ì¶œë ¥
                        src = rel.get('source_entity', '')
                        tgt = rel.get('target_entity', '')
                        print(f"      {src} â†’ {tgt}")
                    if len(resolved_relationships) > 5:
                        print(f"      ... and {len(resolved_relationships) - 5} more")
                    
                    # Neptuneì— ê´€ê³„ ì €ì¥
                    if save_to_neptune and resolved_relationships:
                        # ë©”íƒ€ë°ì´í„° ì œê±° í›„ ì €ì¥
                        clean_rels = []
                        for rel in resolved_relationships:
                            clean_rel = {k: v for k, v in rel.items() if not k.startswith('_')}
                            clean_rels.append(clean_rel)
                        
                        rel_save_result = import_relationships_with_dynamic_label(clean_rels)
                        rel_stats = rel_save_result.get('stats', {})
                        total['relationships_saved'] += rel_stats.get('total', len(clean_rels))
                        total['relationships_existing_in_neptune'] += rel_stats.get('existing', 0)
                        total['relationships_new_in_neptune'] += rel_stats.get('new', 0)
                        
                        # ê¸°ì¡´/ì‹ ê·œ relationship í†µê³„ ì¶œë ¥
                        rel_existing = rel_stats.get('existing', 0)
                        rel_new = rel_stats.get('new', 0)
                        rel_total = rel_stats.get('total', 0)
                        print(f"   ğŸ’¾ Saved {rel_total} relationships to Neptune (DBì— ê¸°ì¡´ ì¡´ì¬: {rel_existing}, ì‹ ê·œ ìƒì„±: {rel_new})")
                
                total['chunks_processed'] += 1
                
        except Exception as e:
            print(f"   âŒ Error: {e}")
            import traceback
            traceback.print_exc()
    
    # ìµœì¢… ê²°ê³¼
    print("\n" + "=" * 60)
    print("ğŸ‰ Pipeline Complete!")
    print("=" * 60)
    print(f"Chunks processed: {total['chunks_processed']}")
    print(f"Entities extracted: {total['entities_extracted']}")
    print(f"  - Matched in OpenSearch: {total['entities_matched']}")
    print(f"  - New (not found): {total['entities_new']}")
    print(f"  - Saved to Neptune: {total['entities_saved']}")
    print(f"    â””â”€ DBì— ê¸°ì¡´ ì¡´ì¬ (ì—…ë°ì´íŠ¸): {total['entities_existing_in_neptune']}")
    print(f"    â””â”€ ì‹ ê·œ ìƒì„±: {total['entities_new_in_neptune']}")
    print(f"Relationships extracted: {total['relationships_extracted']}")
    print(f"  - Saved to Neptune: {total['relationships_saved']}")
    print(f"    â””â”€ DBì— ê¸°ì¡´ ì¡´ì¬ (ì—…ë°ì´íŠ¸): {total['relationships_existing_in_neptune']}")
    print(f"    â””â”€ ì‹ ê·œ ìƒì„±: {total['relationships_new_in_neptune']}")
    
    if save_to_neptune:
        final_stats = get_database_stats()
        print(f"\nğŸ“Š Final Neptune Stats:")
        print(f"  - Total nodes: {final_stats['total_nodes']}")
        print(f"  - Total relationships: {final_stats['total_relationships']}")
    
    return total


def run_entity_check_pipeline(reviews_dir: str = None):
    """
    LLMìœ¼ë¡œ ì—”í‹°í‹° ì¶”ì¶œ í›„ OpenSearchì—ì„œ ë§¤ì¹­ í™•ì¸ë§Œ ìˆ˜í–‰
    (Neptune ì €ì¥ ì—†ìŒ - í…ŒìŠ¤íŠ¸ìš©)
    """
    return run_entity_extraction_pipeline(
        reviews_dir=reviews_dir,
        save_to_neptune=False,
        clean_database=False
    )


if __name__ == "__main__":
    # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (Neptune ì €ì¥ í¬í•¨)
    run_entity_extraction_pipeline(
        reviews_dir="../../data/reviews/DonghoonChoi",  # ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©
        chunk_size=1500,
        chunk_overlap=100,
        clean_database=True,
        save_to_neptune=True
    )
