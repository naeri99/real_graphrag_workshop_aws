"""
Entity Extraction from Chunk Pipeline
Flow:
1. Extract entities from chunk (LLM)
2. Resolve entity names via OpenSearch (synonym matching)
3. Save entities to Neptune
4. Resolve relationship names using cache
5. Save relationships to Neptune
"""
from langchain_text_splitters import RecursiveCharacterTextSplitter
from utils.parse_utils import parse_extraction_output
from utils.helper import (
    get_context_from_review_file, 
    get_all_review_files,
    generate_chunk_hash,
    generate_chunk_id
)
from opensearch.opensearch_con import get_opensearch_client
from opensearch.opensearch_search import resolve_entities, resolve_relationships, delete_chunk_index_opensearch
from neptune.cyper_queries import (
    import_nodes_with_dynamic_label,
    import_relationships_with_dynamic_label,
    delete_all_nodes_and_relationships,
    get_database_stats
)
import time
import json
import os
from pathlib import Path

# ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ ê¸°ì¤€ ë””ë ‰í† ë¦¬
SCRIPT_DIR = Path(__file__).parent.resolve()
DEFAULT_OUTPUT_DIR = SCRIPT_DIR / "step" / "chunkings"


def save_chunk_to_json(chunk_data: dict, output_dir: str = None) -> str:
    """
    Chunk ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    
    Args:
        chunk_data: ì €ì¥í•  chunk ë°ì´í„° (chunk_hash, chunk_id, user_query í¬í•¨)
        output_dir: ì €ì¥í•  ë””ë ‰í† ë¦¬ ê²½ë¡œ
    
    Returns:
        ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
    """
    # ë””ë ‰í† ë¦¬ ìƒì„± (ê¸°ë³¸ê°’: ìŠ¤í¬ë¦½íŠ¸ ê¸°ì¤€)
    if output_dir is None:
        output_dir = DEFAULT_OUTPUT_DIR
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # íŒŒì¼ëª…: chunk_id ì‚¬ìš©
    filename = f"{chunk_data['chunk_id']}.json"
    filepath = os.path.join(output_dir, filename)
    
    # JSON ì €ì¥
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(chunk_data, f, ensure_ascii=False, indent=2)
    
    print(f"   ğŸ’¾ Saved: {filepath}")
    return filepath


def save_all_chunks_to_json(chunks_list: list, output_dir: str = None) -> str:
    """
    ëª¨ë“  chunk ë°ì´í„°ë¥¼ í•˜ë‚˜ì˜ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    
    Args:
        chunks_list: chunk ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        output_dir: ì €ì¥í•  ë””ë ‰í† ë¦¬ ê²½ë¡œ
    
    Returns:
        ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
    """
    if output_dir is None:
        output_dir = DEFAULT_OUTPUT_DIR
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    filepath = output_dir / "all_chunks.json"
    
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(chunks_list, f, ensure_ascii=False, indent=2)
    
    print(f"   ğŸ’¾ Saved all chunks: {filepath}")
    return filepath

def clear_output_directory(output_dir: str = None):
    """
    ì¶œë ¥ ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  íŒŒì¼ì„ ì‚­ì œí•©ë‹ˆë‹¤.
    """
    if output_dir is None:
        output_dir = DEFAULT_OUTPUT_DIR
    dir_path = Path(output_dir)
    if dir_path.exists():
        for file in dir_path.glob("*.json"):
            file.unlink()
        print(f"ğŸ—‘ï¸ Cleared: {output_dir}")


def get_chunk(reviews_dir):
    if reviews_dir:
        from pathlib import Path
        review_files = list(Path(reviews_dir).rglob("*.json"))
    else:
        review_files = get_all_review_files()
    return review_files


def run_chunking(
    reviews_dir: str = None,
    chunk_size: int = 1500,
    chunk_overlap: int = 100,
    output_dir: str = None
):
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì • (ê¸°ë³¸ê°’: ìŠ¤í¬ë¦½íŠ¸ ê¸°ì¤€)
    if output_dir is None:
        output_dir = DEFAULT_OUTPUT_DIR
    output_dir = Path(output_dir)
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì´ˆê¸°í™”
    clear_output_directory(output_dir)
    
    review_files=get_chunk(reviews_dir)
    
    # í…ìŠ¤íŠ¸ ìŠ¤í”Œë¦¬í„°
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    
    for i, review_file in enumerate(review_files, 1):
        print(f"\n{'='*60}")
        print(f"ğŸ“„ [{i}/{len(review_files)}] {review_file.name}")
        print('='*60)
        
        # íŒŒì¼ì—ì„œ ì •ë³´ ì¶”ì¶œ
        _, transcript, movie_id, reviewer = get_context_from_review_file(str(review_file))
        print(f"   ğŸ¬ Movie: {movie_id}, Reviewer: {reviewer}")
        
        # ì²­í‚¹
        chunks = text_splitter.split_text(transcript)
        print(f"   ğŸ“ Chunks: {len(chunks)}")
        
        for j, chunk in enumerate(chunks, 1):
            print(f"\n   --- Chunk {j}/{len(chunks)} ---")
            print(f"f{chunk[:800]}... ìƒëµ ...")
            chunk_hash = generate_chunk_hash(chunk)
            chunk_id = generate_chunk_id(reviewer, chunk_hash)

            # Step 1: chunk ë°ì´í„° êµ¬ì„± ë° ì €ì¥
            save_chunk = {
                "chunk_hash": chunk_hash,
                "chunk_id": chunk_id,
                "user_query": chunk,
                "movie_id": movie_id,
                "reviewer": reviewer,
                "chunk_index": j,
            }
            
            # JSON íŒŒì¼ë¡œ ì €ì¥
            save_chunk_to_json(save_chunk, output_dir)


if __name__ == "__main__":
    # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (Neptune ì €ì¥ í¬í•¨)
    run_chunking(
        reviews_dir="../../data/reviews/DonghoonChoi",  # ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©
        chunk_size=1500,
        chunk_overlap=100
    )
    print(f"ğŸ“ Output directory: {DEFAULT_OUTPUT_DIR}")
