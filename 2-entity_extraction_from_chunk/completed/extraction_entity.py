"""
Entity Extraction from Chunk Pipeline
Flow:
1. Read chunks from ./step/chunkings
2. Extract entities from chunk (LLM)
3. Save entities and relationships back to JSON
"""
import json
import os
from pathlib import Path
from utils.parse_utils import parse_extraction_output
from utils.generate_entity import extract_entities


def read_chunks_from_dir(chunk_dir: str = "./step/chunkings") -> list:
    """
    ./step/chunkings ë””ë ‰í† ë¦¬ì—ì„œ ëª¨ë“  JSON íŒŒì¼ì„ ì½ì–´ì˜µë‹ˆë‹¤.
    
    Args:
        chunk_dir: chunk JSON íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬
    
    Returns:
        chunk ë°ì´í„° ë¦¬ìŠ¤íŠ¸
    """
    chunks = []
    chunk_path = Path(chunk_dir)
    
    if not chunk_path.exists():
        print(f"   âš ï¸ Directory not found: {chunk_dir}")
        return chunks
    
    json_files = list(chunk_path.glob("*.json"))
    
    for json_file in json_files:
        # all_chunks.jsonì€ ì œì™¸ (ê°œë³„ íŒŒì¼ë§Œ ì²˜ë¦¬)
        if json_file.name == "all_chunks.json":
            continue
        
        with open(json_file, 'r', encoding='utf-8') as f:
            chunk_data = json.load(f)
            chunk_data['_filepath'] = str(json_file)  # ì›ë³¸ íŒŒì¼ ê²½ë¡œ ì €ì¥
            chunks.append(chunk_data)
    
    return chunks


def save_chunk_with_entities(chunk_data: dict) -> str:
    """
    ì—”í‹°í‹°/ê´€ê³„ê°€ ì¶”ê°€ëœ chunk ë°ì´í„°ë¥¼ ì›ë³¸ íŒŒì¼ì— ë®ì–´ì”ë‹ˆë‹¤.
    
    Args:
        chunk_data: entities, relationshipsê°€ í¬í•¨ëœ chunk ë°ì´í„°
    
    Returns:
        ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
    """
    filepath = chunk_data.get('_filepath')
    
    # _filepath í•„ë“œ ì œê±° (ë‚´ë¶€ìš©)
    save_data = {k: v for k, v in chunk_data.items() if k != '_filepath'}
    
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(save_data, f, ensure_ascii=False, indent=2)
    
    print(f"   ğŸ’¾ Saved: {filepath}")
    return filepath


def run_entity_extraction_pipeline(
    chunk_dir: str = "./step/chunkings"
):
    """
    ./step/chunkingsì—ì„œ chunk íŒŒì¼ë“¤ì„ ì½ì–´ ì—”í‹°í‹°/ê´€ê³„ë¥¼ ì¶”ì¶œí•˜ê³  ì›ë³¸ íŒŒì¼ì— ë®ì–´ì”ë‹ˆë‹¤.
    
    Args:
        chunk_dir: chunk JSON íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬
    """
    # chunk íŒŒì¼ë“¤ ì½ê¸°
    chunks = read_chunks_from_dir(chunk_dir)
    print(f"   ğŸ“ Loaded Chunks: {len(chunks)}")
    
    if not chunks:
        print("   âš ï¸ No chunks found to process")
        return
    
    for j, chunk in enumerate(chunks, 1):
        print(f"\n   --- Chunk {j}/{len(chunks)} ---")
        print(f"   ğŸ“„ ID: {chunk.get('chunk_id', 'unknown')}")
        print(f"   ğŸ“ Chunk: {chunk.get('user_query', '')[:400]}...")
        
        # Step 1: LLMìœ¼ë¡œ ì—”í‹°í‹°/ê´€ê³„ ì¶”ì¶œ
        print(f"\n   --- Chunk Reformation ---")

        result = extract_entities({"user_query": chunk.get('user_query', '')})
        entities, relationships = parse_extraction_output(result)
        
        # chunkì— ì—”í‹°í‹°/ê´€ê³„ ì¶”ê°€
        chunk["entities"] = entities
        chunk["relationships"] = relationships
        
        print(f"   âœ… Entities: {len(entities)}, Relationships: {len(relationships)}")
        
        # Step 2: ì›ë³¸ íŒŒì¼ì— ë®ì–´ì“°ê¸°
        save_chunk_with_entities(chunk)
        print(f"   ğŸ’¾ Saved: {chunk.get('_filepath')}")
    
    print(f"\n{'='*60}")
    print(f"âœ… Entity extraction completed for {len(chunks)} chunks")


if __name__ == "__main__":
    run_entity_extraction_pipeline(
        chunk_dir="./step/chunkings"
    )
