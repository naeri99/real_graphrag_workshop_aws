"""
ë™ì˜ì–´ ì„¤ì • í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
"""
import sys
import os
import json

# ìƒìœ„ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from opensearch.opensearh_search import (
    get_synonyms,
    analyze_text_with_synonyms,
    test_synonym_expansion
)


def print_separator(title):
    print("\n" + "=" * 60)
    print(f" {title}")
    print("=" * 60)


def test_get_synonyms(index_name="entities"):
    """ë™ì˜ì–´ ì„¤ì • í™•ì¸ í…ŒìŠ¤íŠ¸"""
    print_separator("ë™ì˜ì–´ ì„¤ì • í™•ì¸")
    
    synonym_info = get_synonyms(index_name)
    
    if synonym_info:
        print(f"ì¸ë±ìŠ¤: {synonym_info['index_name']}")
        
        print("\nğŸ“Œ ë™ì˜ì–´ í•„í„°:")
        if synonym_info['filters']:
            for name, config in synonym_info['filters'].items():
                print(f"  - {name}:")
                print(f"    íƒ€ì…: {config.get('type')}")
                synonyms = config.get('synonyms', [])
                if synonyms:
                    print(f"    ë™ì˜ì–´ (ì´ {len(synonyms)}ê°œ):")
                    for syn in synonyms:  # ì „ì²´ ì¶œë ¥
                        print(f"      â€¢ {syn}")
        else:
            print("  ë™ì˜ì–´ í•„í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        print("\nğŸ“Œ ë¶„ì„ê¸°:")
        for name, config in synonym_info['analyzers'].items():
            print(f"  - {name}:")
            print(f"    í† í¬ë‚˜ì´ì €: {config.get('tokenizer', 'N/A')}")
            print(f"    í•„í„°: {config.get('filter', [])}")
    else:
        print("âŒ ë™ì˜ì–´ ì„¤ì •ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")


def test_analyze_text(index_name="entities"):
    """í…ìŠ¤íŠ¸ ë¶„ì„ í…ŒìŠ¤íŠ¸"""
    print_separator("í…ìŠ¤íŠ¸ ë¶„ì„ í…ŒìŠ¤íŠ¸")
    
    test_texts = [
        "ë¬´ë¥µ"
    ]
    
    for text in test_texts:
        print(f"\nğŸ” '{text}' ë¶„ì„ ê²°ê³¼:")
        tokens = analyze_text_with_synonyms(text, index_name)
        
        if tokens:
            for token in tokens:
                print(f"  - {token['token']} (íƒ€ì…: {token['type']}, ìœ„ì¹˜: {token['position']})")
        else:
            print("  í† í° ì—†ìŒ")


def test_synonym_expansion_comparison(index_name="entities"):
    """ë™ì˜ì–´ í™•ì¥ ë¹„êµ í…ŒìŠ¤íŠ¸"""
    print_separator("ë™ì˜ì–´ í™•ì¥ ë¹„êµ")
    
    test_texts = ["ë¬´ë¥µ"]
    
    for text in test_texts:
        print(f"\nğŸ” '{text}' ë¶„ì„ê¸°ë³„ ê²°ê³¼:")
        results = test_synonym_expansion(text, index_name)
        
        for analyzer, tokens in results.items():
            print(f"  [{analyzer}]: {', '.join(tokens)}")


def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    # ê¸°ë³¸ ì¸ë±ìŠ¤ ì´ë¦„ (í•„ìš”ì‹œ ë³€ê²½)
    index_name=  "entities"
    try:
        # 1. ë™ì˜ì–´ ì„¤ì • í™•ì¸
        test_get_synonyms(index_name)
        
        # 2. í…ìŠ¤íŠ¸ ë¶„ì„ í…ŒìŠ¤íŠ¸
        test_analyze_text(index_name)
        
        # 3. ë™ì˜ì–´ í™•ì¥ ë¹„êµ
        test_synonym_expansion_comparison(index_name)
        
        print("\n" + "=" * 60)
        print(" âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        print("=" * 60)
        
    except Exception as e:
        print(f"\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
