"""
Entity Setting - movie_cast ê¸°ë°˜ ë™ì˜ì–´ ì¶”ì¶œ ë° OpenSearch ì €ì¥ íŒŒì´í”„ë¼ì¸

ë°ì´í„° ì†ŒìŠ¤:
  - movie_cast/*.json â†’ ì˜í™” ì •ë³´(context): movie_title, director, cast
  - real/*/*.json (review ê²½ë¡œ) â†’ refined_transcript (chunk ëŒ€ìƒ)

íë¦„:
1. movie_cast JSONì—ì„œ ì˜í™”ë³„ context ìƒì„±
2. review ê²½ë¡œì˜ refined_transcriptë¥¼ ì²­í¬ë¡œ ë¶„í• 
3. ì²­í¬ì—ì„œ ë™ì˜ì–´ ì¶”ì¶œ
4. OpenSearchì—ì„œ ì—”í‹°í‹° ê²€ìƒ‰ í›„ ë™ì˜ì–´ ì¶”ê°€
"""
import json, glob, os, sys

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from opensearch.opensearch_con import get_opensearch_client
from opensearch.opensearh_search import find_entity_opensearch
from utils.parse_utils import parse_mixed_synonym_output
from utils.generate_entity import extract_synonym
from utils.synonym import (
    clean_entities_whitespace,
    merge_synonyms_with_set,
    update_entity_synonyms
)
from langchain_text_splitters import RecursiveCharacterTextSplitter


CAST_DIR = os.path.normpath(os.path.join(
    os.path.dirname(os.path.abspath(__file__)),
    os.pardir, os.pardir, "data", "raw_csv", "movie_cast"
))


# ============================================================
# movie_cast ê¸°ë°˜ context ìƒì„±
# ============================================================
def build_context_from_cast(cast_data: dict) -> str:
    """movie_cast JSONì—ì„œ context ë¬¸ìì—´ ìƒì„±"""
    title = cast_data["movie_title"]
    directors = cast_data.get("director", [])
    cast = cast_data.get("cast", [])

    parts = [f"ì˜í™” {title}ì˜ ì£¼ìš” ë“±ì¥ì¸ë¬¼ê³¼ ë°°ìš° ì •ë³´:", ""]
    for c in cast:
        parts.append(f"- {c['character']}: {c['actor']}ì´ ì—°ê¸°í•œ ìºë¦­í„°")

    parts.append("")
    parts.append(f"ì˜í™”: {title}")
    if directors:
        parts.append(f"ê°ë…: {directors[0]['name']}")
    parts.append(f"ì´ {len(cast)}ëª…ì˜ ë°°ìš°ê°€ {len(cast)}ê°œì˜ ìºë¦­í„°ë¥¼ ì—°ê¸°í–ˆìŠµë‹ˆë‹¤.")

    return "\n".join(parts)


def load_refined_transcripts(review_paths: list) -> list:
    """review ê²½ë¡œë“¤ì—ì„œ refined_transcript ë¡œë“œ, ì—†ìœ¼ë©´ ìŠ¤í‚µ. (path, transcript) íŠœí”Œ ë°˜í™˜"""
    results = []
    for rpath in review_paths:
        try:
            with open(rpath, "r", encoding="utf-8") as f:
                data = json.load(f)
            rt = data.get("refined_transcript", "")
            if rt:
                results.append((rpath, rt))
            else:
                print(f"      â­ï¸  refined_transcript ì—†ìŒ, ìŠ¤í‚µ: {os.path.basename(rpath)}")
        except Exception as e:
            print(f"      â­ï¸  íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨, ìŠ¤í‚µ: {os.path.basename(rpath)} ({e})")
    return results


# ============================================================
# ì²­í‚¹ í•¨ìˆ˜ (ê¸°ì¡´ê³¼ ë™ì¼)
# ============================================================
def chunk_text(text: str, chunk_size: int = 1500, chunk_overlap: int = 100) -> list:
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size, chunk_overlap=chunk_overlap
    )
    return text_splitter.split_text(text)


# ============================================================
# íŒŒì´í”„ë¼ì¸ í•¨ìˆ˜ë“¤ (ê¸°ì¡´ í•µì‹¬ ë¡œì§ ê·¸ëŒ€ë¡œ)
# ============================================================
def find_synonyms_from_chunk(chunk: str, context: str) -> list:
    """ì²­í¬ì—ì„œ ë™ì˜ì–´ ì°¾ê¸° (ì¶”ì¶œ + ê³µë°± ì œê±°)"""
    result = extract_synonym({"movie_context": context, "movie_chunk": chunk})
    entities = parse_mixed_synonym_output(result)
    if not entities:
        return []
    return clean_entities_whitespace(entities)


def find_entity_from_opensearch(opensearch_client, entity_name: str, index_name: str = "entities"):
    """OpenSearchì—ì„œ ì—”í‹°í‹° ê²€ìƒ‰"""
    return find_entity_opensearch(opensearch_client, entity_name, index_name)


def add_synonyms_to_entity(opensearch_client, existing_entity: dict, new_synonyms: list, index_name: str = "entities") -> dict:
    """ì—”í‹°í‹°ì— ë™ì˜ì–´ ì¶”ê°€ (ë³‘í•© + ì €ì¥)"""
    existing_synonyms = existing_entity['entity'].get('synonym', [])
    merged_synonyms = merge_synonyms_with_set(existing_synonyms, new_synonyms)
    merged_synonyms_list = list(merged_synonyms) if isinstance(merged_synonyms, set) else merged_synonyms
    success = update_entity_synonyms(opensearch_client, existing_entity['id'], merged_synonyms_list, index_name)
    return {'success': success, 'merged_synonyms': merged_synonyms_list}


# ============================================================
# ë‹¨ì¼ ì˜í™” ì²˜ë¦¬
# ============================================================
def process_single_movie(opensearch_client, cast_data: dict, stats: dict):
    """
    ë‹¨ì¼ movie_cast JSON ì²˜ë¦¬

    íë¦„:
    1. cast_dataì—ì„œ context ìƒì„±
    2. review ê²½ë¡œì˜ refined_transcript ë¡œë“œ
    3. transcriptë¥¼ ì²­í¬ë¡œ ë¶„í• 
    4. ê° ì²­í¬ì—ì„œ ë™ì˜ì–´ ì¶”ì¶œ
    5. OpenSearchì—ì„œ ì—”í‹°í‹° ê²€ìƒ‰ í›„ ë™ì˜ì–´ ì¶”ê°€
    """
    title = cast_data["movie_title"]
    print(f"\nğŸ¬ ì˜í™” ì²˜ë¦¬: {title}")

    # Step 1: context ìƒì„±
    context = build_context_from_cast(cast_data)
    print(f"   ğŸ“ Context ìƒì„± ì™„ë£Œ")

    # Step 2: review ê²½ë¡œì—ì„œ refined_transcript ë¡œë“œ
    review_paths = cast_data.get("review", [])
    transcript_pairs = load_refined_transcripts(review_paths)
    if not transcript_pairs:
        print(f"   âš ï¸ refined_transcript ì—†ìŒ, ê±´ë„ˆëœ€")
        return

    print(f"   ğŸ“„ {len(transcript_pairs)}ê°œ ë¦¬ë·° transcript ë¡œë“œ")

    # Step 3: ê° transcriptë¥¼ ì²­í¬ë¡œ ë¶„í•  í›„ ì²˜ë¦¬
    for t_idx, (rpath, transcript) in enumerate(transcript_pairs, 1):
        review_filename = os.path.basename(rpath)
        chunks = chunk_text(transcript)
        print(f"   ğŸ“„ ë¦¬ë·° [{t_idx}] {review_filename} â†’ {len(chunks)}ê°œ ì²­í¬")

        # Step 4-5: ê° ì²­í¬ ì²˜ë¦¬ (ê¸°ì¡´ í•µì‹¬ ë¡œì§ ê·¸ëŒ€ë¡œ)
        for c_idx, chunk in enumerate(chunks, 1):
            print(f"      ğŸ“¦ chunk [{c_idx}/{len(chunks)}] ({len(chunk)}ì) | {review_filename}")
            try:
                entities = find_synonyms_from_chunk(chunk, context)
                if not entities:
                    print(f"      â­ï¸  ë™ì˜ì–´ ì—†ìŒ")
                    continue

                for entity_data in entities:
                    entity_name = entity_data['entity_name']
                    new_synonyms = entity_data['synonyms']
                    stats['total_entities'] += 1

                    existing_entity = find_entity_from_opensearch(opensearch_client, entity_name)
                    if not existing_entity:
                        stats['not_found'] += 1
                        continue

                    result = add_synonyms_to_entity(opensearch_client, existing_entity, new_synonyms)
                    if result['success']:
                        stats['updated'] += 1
                        print(f"      âœ… {entity_name}: ë™ì˜ì–´ ì—…ë°ì´íŠ¸")
                    else:
                        stats['failed'] += 1
            except Exception as e:
                print(f"      âŒ chunk ì²˜ë¦¬ ì—ëŸ¬: {e}")
                continue


# ============================================================
# ë©”ì¸ íŒŒì´í”„ë¼ì¸
# ============================================================
def run_cast_synonym_pipeline():
    """
    movie_cast ê¸°ë°˜ ë™ì˜ì–´ ì¶”ì¶œ íŒŒì´í”„ë¼ì¸

    íë¦„:
    1. movie_cast ë””ë ‰í† ë¦¬ì—ì„œ ëª¨ë“  JSON ë¡œë“œ
    2. ê° ì˜í™”ë³„ë¡œ context ìƒì„± + reviewì˜ refined_transcript ì²­í‚¹
    3. ì²­í¬ì—ì„œ ë™ì˜ì–´ ì¶”ì¶œ
    4. OpenSearchì—ì„œ ì—”í‹°í‹° ê²€ìƒ‰ í›„ ë™ì˜ì–´ ì¶”ê°€
    """
    print("ğŸš€ movie_cast ê¸°ë°˜ ë™ì˜ì–´ ì¶”ì¶œ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
    print("=" * 60)

    opensearch_client = get_opensearch_client()

    # Step 1: movie_cast JSON íŒŒì¼ ëª©ë¡
    cast_files = sorted(glob.glob(os.path.join(CAST_DIR, "*.json")))
    print(f"ğŸ“‚ ì´ {len(cast_files)}ê°œ ì˜í™” ë°œê²¬")

    stats = {
        'total_entities': 0, 'updated': 0,
        'not_found': 0, 'failed': 0, 'movies_processed': 0
    }

    # Step 2-4: ê° ì˜í™” ì²˜ë¦¬
    for file_idx, cast_file in enumerate(cast_files, 1):
        movie_name = os.path.splitext(os.path.basename(cast_file))[0]
        print(f"\n{'='*60}")
        print(f"ğŸ“ [{file_idx}/{len(cast_files)}] {movie_name}")

        try:
            with open(cast_file, "r", encoding="utf-8") as f:
                cast_data = json.load(f)
            process_single_movie(opensearch_client, cast_data, stats)
            stats['movies_processed'] += 1
        except Exception as e:
            print(f"   âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 60)
    print("ğŸ¯ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
    print(f"   ì²˜ë¦¬ëœ ì˜í™”: {stats['movies_processed']}ê°œ")
    print(f"   ì „ì²´ ì—”í‹°í‹°: {stats['total_entities']}ê°œ")
    print(f"   ì—…ë°ì´íŠ¸ ì„±ê³µ: {stats['updated']}ê°œ")
    print(f"   ì—”í‹°í‹° ì—†ìŒ: {stats['not_found']}ê°œ")
    print(f"   ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {stats['failed']}ê°œ")

    return stats


# ============================================================
# ì‹¤í–‰
# ============================================================
if __name__ == "__main__":
    run_cast_synonym_pipeline()
