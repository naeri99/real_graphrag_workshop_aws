"""
Entity Setting - ì—”í‹°í‹° ë™ì˜ì–´ ì¶”ì¶œ ë° OpenSearch ì €ì¥ íŒŒì´í”„ë¼ì¸

íë¦„:
1. ì²­í¬ì—ì„œ ì—”í‹°í‹° ê¸°ì¤€ìœ¼ë¡œ ë™ì˜ì–´ ì¶”ì¶œ
2. OpenSearchì—ì„œ ì—”í‹°í‹° ì´ë¦„ìœ¼ë¡œ ê²€ìƒ‰
3. ê¸°ì¡´ ë™ì˜ì–´ì™€ ìƒˆ ë™ì˜ì–´ ë³‘í•© (strip() + set)
4. OpenSearchì— ì €ì¥
"""
from opensearch.opensearch_index_setting import delete_index, define_entity_index
from opensearch.opensearch_con import get_opensearch_client
from opensearch.opensearh_search import find_entity_opensearch
from utils.parse_utils import parse_mixed_synonym_output
from utils.generate_entity import extract_synonym
from utils.synonym import (
    clean_entities_whitespace,
    merge_synonyms_with_set,
    update_entity_synonyms
)
from utils.movie_context import get_context_from_review_file, get_all_review_files
from langchain_text_splitters import RecursiveCharacterTextSplitter
import os




# ============================================================
# ì²­í‚¹ í•¨ìˆ˜
# ============================================================
def chunk_text(text: str, chunk_size: int = 1500, chunk_overlap: int = 100) -> list:
    """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• """
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    return text_splitter.split_text(text)


# ============================================================
# íŒŒì´í”„ë¼ì¸ í•¨ìˆ˜ë“¤
# ============================================================
def find_synonyms_from_chunk(chunk: str, context: str) -> list:
    """Step 2: ì²­í¬ì—ì„œ ë™ì˜ì–´ ì°¾ê¸° (ì¶”ì¶œ + ê³µë°± ì œê±°)"""
    result = extract_synonym({"movie_context": context, "movie_chunk": chunk})
    entities = parse_mixed_synonym_output(result)
    if not entities:
        return []
    return clean_entities_whitespace(entities)


def find_entity_from_opensearch(opensearch_client, entity_name: str, index_name: str = "entities"):
    """Step 3: OpenSearchì—ì„œ ì—”í‹°í‹° ê²€ìƒ‰"""
    return find_entity_opensearch(opensearch_client, entity_name, index_name)


def add_synonyms_to_entity(opensearch_client, existing_entity: dict, new_synonyms: list, index_name: str = "entities") -> dict:
    """Step 4: ì—”í‹°í‹°ì— ë™ì˜ì–´ ì¶”ê°€ (ë³‘í•© + ì €ì¥)"""
    existing_synonyms = existing_entity['entity'].get('synonym', [])
    merged_synonyms = merge_synonyms_with_set(existing_synonyms, new_synonyms)
    
    # setì„ listë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥
    merged_synonyms_list = list(merged_synonyms) if isinstance(merged_synonyms, set) else merged_synonyms
    
    success = update_entity_synonyms(opensearch_client, existing_entity['id'], merged_synonyms_list, index_name)
    return {'success': success, 'merged_synonyms': merged_synonyms_list}


def print_final_stats(stats: dict):
    """ìµœì¢… ê²°ê³¼ ì¶œë ¥"""
    print("\n" + "=" * 60)
    print("ğŸ¯ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
    print(f"   ì „ì²´ ì—”í‹°í‹°: {stats['total_entities']}ê°œ")
    print(f"   ì—…ë°ì´íŠ¸ ì„±ê³µ: {stats['updated']}ê°œ")
    print(f"   ì—”í‹°í‹° ì—†ìŒ: {stats['not_found']}ê°œ")
    print(f"   ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {stats['failed']}ê°œ")


# ============================================================
# ë¦¬ë·° ë””ë ‰í† ë¦¬ ê¸°ë°˜ íŒŒì´í”„ë¼ì¸
# ============================================================
def process_single_review_file(opensearch_client, review_filepath: str, stats: dict):
    """
    ë‹¨ì¼ ë¦¬ë·° íŒŒì¼ ì²˜ë¦¬
    
    íë¦„:
    1. ë¦¬ë·° íŒŒì¼ì—ì„œ context, transcript ì¶”ì¶œ
    2. transcriptë¥¼ ì²­í¬ë¡œ ë¶„í• 
    3. ê° ì²­í¬ì—ì„œ ë™ì˜ì–´ ì¶”ì¶œ
    4. OpenSearchì—ì„œ ì—”í‹°í‹° ê²€ìƒ‰ í›„ ë™ì˜ì–´ ì¶”ê°€
    """
    filename = os.path.basename(review_filepath)
    print(f"\nğŸ“„ íŒŒì¼ ì²˜ë¦¬: {filename}")
    
    # Step 1: context, transcript ì¶”ì¶œ
    context, transcript = get_context_from_review_file(review_filepath)
    print(f"   ğŸ“ Context ìƒì„± ì™„ë£Œ")
    
    # Step 2: transcriptë¥¼ ì²­í¬ë¡œ ë¶„í• 
    chunks = chunk_text(transcript)
    print(f"   ğŸ“¦ {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
    
    # Step 3-4: ê° ì²­í¬ ì²˜ë¦¬
    for i, chunk in enumerate(chunks, 1):
        # ì²­í¬ì—ì„œ ë™ì˜ì–´ ì°¾ê¸°
        entities = find_synonyms_from_chunk(chunk, context)
        if not entities:
            continue
        
        for entity_data in entities:
            entity_name = entity_data['entity_name']
            new_synonyms = entity_data['synonyms']
            stats['total_entities'] += 1
            
            # OpenSearchì—ì„œ ì—”í‹°í‹° ê²€ìƒ‰
            existing_entity = find_entity_from_opensearch(opensearch_client, entity_name)
            if not existing_entity:
                stats['not_found'] += 1
                continue
            
            # ì—”í‹°í‹°ì— ë™ì˜ì–´ ì¶”ê°€
            result = add_synonyms_to_entity(opensearch_client, existing_entity, new_synonyms)
            if result['success']:
                stats['updated'] += 1
                print(f"   âœ… {entity_name}: ë™ì˜ì–´ ì—…ë°ì´íŠ¸")
            else:
                stats['failed'] += 1


def run_directory_pipeline(reviews_dir: str = None):
    """
    ë¦¬ë·° ë””ë ‰í† ë¦¬ ê¸°ë°˜ ë™ì˜ì–´ ì¶”ì¶œ íŒŒì´í”„ë¼ì¸
    
    Args:
        reviews_dir: ë¦¬ë·° íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ ê²½ë¡œ
                    ê¸°ë³¸ê°’: /home/ec2-user/real_graphrag_workshop_aws/data/reviews/DonghoonChoi
    
    íë¦„:
    1. ë””ë ‰í† ë¦¬ì—ì„œ ëª¨ë“  ë¦¬ë·° íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    2. ê° íŒŒì¼ì—ì„œ context, transcript ì¶”ì¶œ
    3. transcriptë¥¼ ì²­í¬ë¡œ ë¶„í• 
    4. ì²­í¬ì—ì„œ ë™ì˜ì–´ ì¶”ì¶œ
    5. OpenSearchì—ì„œ ì—”í‹°í‹° ê²€ìƒ‰
    6. ì—”í‹°í‹°ì— ë™ì˜ì–´ ì¶”ê°€
    """
    print("ğŸš€ ë¦¬ë·° ë””ë ‰í† ë¦¬ ê¸°ë°˜ ë™ì˜ì–´ ì¶”ì¶œ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
    print("=" * 60)
    
    opensearch_client = get_opensearch_client()
    
    # Step 1: ë¦¬ë·° íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    if reviews_dir:
        from pathlib import Path
        review_files = list(Path(reviews_dir).glob("*.json"))
    else:
        review_files = get_all_review_files()
    
    print(f"ğŸ“‚ ì´ {len(review_files)}ê°œ ë¦¬ë·° íŒŒì¼ ë°œê²¬")
    
    stats = {'total_entities': 0, 'updated': 0, 'not_found': 0, 'failed': 0, 'files_processed': 0}
    
    # Step 2-6: ê° íŒŒì¼ ì²˜ë¦¬
    for file_idx, review_file in enumerate(review_files, 1):
        print(f"\n{'='*60}")
        print(f"ğŸ“ [{file_idx}/{len(review_files)}] íŒŒì¼ ì²˜ë¦¬ ì¤‘")
        
        try:
            process_single_review_file(opensearch_client, str(review_file), stats)
            stats['files_processed'] += 1
        except Exception as e:
            print(f"   âŒ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 60)
    print("ğŸ¯ ë””ë ‰í† ë¦¬ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
    print(f"   ì²˜ë¦¬ëœ íŒŒì¼: {stats['files_processed']}ê°œ")
    print(f"   ì „ì²´ ì—”í‹°í‹°: {stats['total_entities']}ê°œ")
    print(f"   ì—…ë°ì´íŠ¸ ì„±ê³µ: {stats['updated']}ê°œ")
    print(f"   ì—”í‹°í‹° ì—†ìŒ: {stats['not_found']}ê°œ")
    print(f"   ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {stats['failed']}ê°œ")
    
    return stats


# ============================================================
# ì‹¤í–‰
# ============================================================
if __name__ == "__main__":
    # ë””ë ‰í† ë¦¬ ê¸°ë°˜ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    run_directory_pipeline()
