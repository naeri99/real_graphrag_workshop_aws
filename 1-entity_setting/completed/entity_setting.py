"""
Entity Setting - ì—”í‹°í‹° ë™ì˜ì–´ ì¶”ì¶œ ë° OpenSearch ì €ì¥ íŒŒì´í”„ë¼ì¸

íë¦„:
1. ì²­í¬ì—ì„œ ì—”í‹°í‹° ê¸°ì¤€ìœ¼ë¡œ ë™ì˜ì–´ ì¶”ì¶œ
2. OpenSearchì—ì„œ ì—”í‹°í‹° ì´ë¦„ìœ¼ë¡œ ê²€ìƒ‰
3. ê¸°ì¡´ ë™ì˜ì–´ì™€ ìƒˆ ë™ì˜ì–´ ë³‘í•© (strip() + set)
4. OpenSearchì— ì €ì¥
"""
from opensearch.opensearch_index_setting import delete_index, define_entity_index
from opensearch.opensearch_con import get_opensearch_client
from opensearch.opensearh_search import find_entity_opensearch
from utils.read_files import load_json_from_list
from utils.parse_utils import parse_mixed_synonym_output
from utils.generate_entity import extract_synonym
from utils.synonym import (
    clean_entities_whitespace,
    merge_synonyms_with_set,
    update_entity_synonyms
)
from langchain_text_splitters import RecursiveCharacterTextSplitter


# ============================================================
# OpenSearch ì¸ë±ìŠ¤ ì´ˆê¸°í™”
# ============================================================
opensearch_conn = get_opensearch_client()

try:
    delete_index(opensearch_conn, "entities")
except:
    print("no entities")

define_entity_index(opensearch_conn, "entities")


# ============================================================
# ì»¨í…ìŠ¤íŠ¸ ìƒì„±
# ============================================================
def make_inception_cast_context() -> str:
    """ì¸ì…‰ì…˜ ìºìŠ¤íŠ¸ ì»¨í…ìŠ¤íŠ¸ ìƒì„±"""
    cast = [
        ("Leonardo DiCaprio", "Dom Cobb"),
        ("Joseph Gordon-Levitt", "Arthur"),
        ("Ellen Page", "Ariadne"),
        ("Tom Hardy", "Eames"),
        ("Ken Watanabe", "Saito"),
        ("Dileep Rao", "Yusuf"),
        ("Cillian Murphy", "Robert Michael Fischer"),
        ("Tom Berenger", "Peter Browning"),
        ("Marion Cotillard", "Mal Cobb"),
        ("Pete Postlethwaite", "Maurice Fischer"),
        ("Michael Caine", "Professor Miles"),
        ("Lukas Haas", "Nash")
    ]
    
    context_parts = ["ì˜í™” ì¸ì…‰ì…˜ì˜ ì£¼ìš” ë“±ì¥ì¸ë¬¼ê³¼ ë°°ìš° ì •ë³´:", ""]
    context_parts.extend([f"- {char}: {actor}ì´ ì—°ê¸°í•œ ìºë¦­í„°" for actor, char in cast])
    context_parts.extend(["", "ì˜í™”: ì¸ì…‰ì…˜", "ë¦¬ë·°ì–´: reviwerman", "ê°ë…: Christopher Nolan"])
    context_parts.append(f"ì´ {len(cast)}ëª…ì˜ ë°°ìš°ê°€ {len(cast)}ê°œì˜ ìºë¦­í„°ë¥¼ ì—°ê¸°í–ˆìŠµë‹ˆë‹¤.")
    
    return "\n".join(context_parts)


# ============================================================
# íŒŒì´í”„ë¼ì¸ í•¨ìˆ˜ë“¤
# ============================================================
def load_and_chunk_data(file_path: str, chunk_size: int = 1500, chunk_overlap: int = 100) -> list:
    """ë°ì´í„° ë¡œë“œ ë° ì²­í‚¹"""
    result = load_json_from_list(file_path)
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    
    chunks = []
    for item in result:
        chunks.extend(text_splitter.split_text(item["data"]))
    return chunks


def init_pipeline(file_path: str) -> tuple:
    """Step 1: íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” - ë°ì´í„° ë¡œë“œ ë° ì²­í‚¹"""
    print("\nğŸ“‚ Step 1: ë°ì´í„° ë¡œë“œ ë° ì²­í‚¹")
    chunks = load_and_chunk_data(file_path)
    print(f"   ì´ {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
    
    stats = {'total_entities': 0, 'updated': 0, 'not_found': 0, 'failed': 0}
    return chunks, stats


def find_synonyms_from_chunk(chunk: str, context: str) -> list:
    """Step 2: ì²­í¬ì—ì„œ ë™ì˜ì–´ ì°¾ê¸° (ì¶”ì¶œ + ê³µë°± ì œê±°)"""
    result = extract_synonym({"movie_context": context, "movie_chunk": chunk})
    entities = parse_mixed_synonym_output(result)
    if not entities:
        return []
    return clean_entities_whitespace(entities)


def find_entity_from_opensearch(opensearch_client, entity_name: str, index_name: str = "entities"):
    """Step 3: OpenSearchì—ì„œ ì—”í‹°í‹° ê²€ìƒ‰"""
    return find_entity_opensearch(opensearch_client, entity_name, index_name)


def add_synonyms_to_entity(opensearch_client, existing_entity: dict, new_synonyms: list, index_name: str = "entities") -> dict:
    """Step 4: ì—”í‹°í‹°ì— ë™ì˜ì–´ ì¶”ê°€ (ë³‘í•© + ì €ì¥)"""
    existing_synonyms = existing_entity['entity'].get('synonym', [])
    merged_synonyms = merge_synonyms_with_set(existing_synonyms, new_synonyms)
    
    success = update_entity_synonyms(opensearch_client, existing_entity['id'], merged_synonyms, index_name)
    return {'success': success, 'merged_synonyms': merged_synonyms}


def print_final_stats(stats: dict):
    """ìµœì¢… ê²°ê³¼ ì¶œë ¥"""
    print("\n" + "=" * 60)
    print("ğŸ¯ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
    print(f"   ì „ì²´ ì—”í‹°í‹°: {stats['total_entities']}ê°œ")
    print(f"   ì—…ë°ì´íŠ¸ ì„±ê³µ: {stats['updated']}ê°œ")
    print(f"   ì—”í‹°í‹° ì—†ìŒ: {stats['not_found']}ê°œ")
    print(f"   ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {stats['failed']}ê°œ")


# ============================================================
# ë©”ì¸ íŒŒì´í”„ë¼ì¸
# ============================================================
def run_synonym_pipeline():
    """
    ë™ì˜ì–´ ì¶”ì¶œ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    
    íë¦„:
    1. ë°ì´í„° ë¡œë“œ ë° ì²­í‚¹
    2. ì²­í¬ì—ì„œ ë™ì˜ì–´ ì°¾ê¸°
    3. OpenSearchì—ì„œ ì—”í‹°í‹° ê²€ìƒ‰
    4. ì—”í‹°í‹°ì— ë™ì˜ì–´ ì¶”ê°€
    """
    print("ğŸš€ ë™ì˜ì–´ ì¶”ì¶œ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
    print("=" * 60)
    
    opensearch_client = get_opensearch_client()
    context = make_inception_cast_context()
    
    # Step 1: ì´ˆê¸°í™”
    chunks, stats = init_pipeline("./data/inception/list.txt")
    
    for i, chunk in enumerate(chunks, 1):
        print(f"\n{'='*60}")
        print(f"ğŸ“ Chunk {i}/{len(chunks)} ì²˜ë¦¬ ì¤‘")
        
        # Step 2: ì²­í¬ì—ì„œ ë™ì˜ì–´ ì°¾ê¸°
        entities = find_synonyms_from_chunk(chunk, context)
        if not entities:
            print("   âš ï¸ ì¶”ì¶œëœ ì—”í‹°í‹° ì—†ìŒ")
            continue
        print(f"   ğŸ” {len(entities)}ê°œ ì—”í‹°í‹° ë™ì˜ì–´ ì¶”ì¶œ ì™„ë£Œ")
        
        for entity_data in entities:
            entity_name = entity_data['entity_name']
            new_synonyms = entity_data['synonyms']
            stats['total_entities'] += 1
            
            # Step 3: OpenSearchì—ì„œ ì—”í‹°í‹° ê²€ìƒ‰
            existing_entity = find_entity_from_opensearch(opensearch_client, entity_name)
            if not existing_entity:
                stats['not_found'] += 1
                print(f"   ğŸ” {entity_name}: ì—”í‹°í‹° ì—†ìŒ")
                continue
            
            # Step 4: ì—”í‹°í‹°ì— ë™ì˜ì–´ ì¶”ê°€
            result = add_synonyms_to_entity(opensearch_client, existing_entity, new_synonyms)
            if result['success']:
                stats['updated'] += 1
                print(f"   âœ… {entity_name}: ë™ì˜ì–´ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
            else:
                stats['failed'] += 1
                print(f"   âŒ {entity_name}: ì—…ë°ì´íŠ¸ ì‹¤íŒ¨")
    
    # ê²°ê³¼ ì¶œë ¥
    print_final_stats(stats)
    return stats


# ============================================================
# ì‹¤í–‰
# ============================================================
if __name__ == "__main__":
    run_synonym_pipeline()
